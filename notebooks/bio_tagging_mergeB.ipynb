{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Summary"
      ],
      "metadata": {
        "id": "UbDlCCmnt8Dc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is the runnable python notebook version of `bio_tagging_mergeB.py`"
      ],
      "metadata": {
        "id": "hqBAneo7xWr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This file is used to BIO tagging on the pure texts for the modelB, which convers information including saluation,\n",
        "party, state, county, and city."
      ],
      "metadata": {
        "id": "fu9fWXcWxUCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output file will be in the format of spaCy Doc file."
      ],
      "metadata": {
        "id": "UtOJoGIlyjcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "special tricks applied: \n",
        "1. use the external file that stores the US states information to tag the abbreviation and full name of the state information.\n",
        "    the abbreviation will be taged as \"STATE\", and the full name will be taged as \"STATE_F\".\n"
      ],
      "metadata": {
        "id": "Zg8WUFxzxmnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "special dependencies:\n",
        "1. the external file that stores the US states information `state.csv`."
      ],
      "metadata": {
        "id": "uZXzPph_MM5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "DhdO0xCLt88N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BhrcO56Ot4a6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import spacy\n",
        "from spacy.util import filter_spans\n",
        "from spacy.tokens import Doc, DocBin\n",
        "import collections\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "yR_HgmGvuF17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the Cicero data\n",
        "cicero_df = pd.read_csv('/content/drive/MyDrive/Cicero/cicero_officials_sample_2022-09-08.csv',\\\n",
        "                  error_bad_lines=False)"
      ],
      "metadata": {
        "id": "M569HBvHuUye",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0d54948-36cd-4b3b-8ca1-8f9817df5f90"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "b'Skipping line 13: expected 55 fields, saw 60\\nSkipping line 34: expected 55 fields, saw 59\\nSkipping line 110: expected 55 fields, saw 60\\nSkipping line 125: expected 55 fields, saw 60\\nSkipping line 127: expected 55 fields, saw 60\\nSkipping line 171: expected 55 fields, saw 60\\nSkipping line 175: expected 55 fields, saw 65\\nSkipping line 181: expected 55 fields, saw 60\\nSkipping line 196: expected 55 fields, saw 60\\nSkipping line 197: expected 55 fields, saw 58\\nSkipping line 206: expected 55 fields, saw 60\\nSkipping line 220: expected 55 fields, saw 60\\nSkipping line 261: expected 55 fields, saw 60\\nSkipping line 272: expected 55 fields, saw 63\\nSkipping line 279: expected 55 fields, saw 60\\nSkipping line 353: expected 55 fields, saw 58\\nSkipping line 356: expected 55 fields, saw 59\\nSkipping line 407: expected 55 fields, saw 57\\nSkipping line 429: expected 55 fields, saw 60\\nSkipping line 446: expected 55 fields, saw 69\\nSkipping line 507: expected 55 fields, saw 60\\nSkipping line 528: expected 55 fields, saw 60\\nSkipping line 529: expected 55 fields, saw 56\\nSkipping line 541: expected 55 fields, saw 60\\nSkipping line 556: expected 55 fields, saw 56\\nSkipping line 570: expected 55 fields, saw 75\\nSkipping line 572: expected 55 fields, saw 60\\nSkipping line 582: expected 55 fields, saw 60\\nSkipping line 583: expected 55 fields, saw 60\\nSkipping line 588: expected 55 fields, saw 78\\nSkipping line 666: expected 55 fields, saw 62\\nSkipping line 737: expected 55 fields, saw 60\\nSkipping line 763: expected 55 fields, saw 79\\nSkipping line 771: expected 55 fields, saw 60\\nSkipping line 785: expected 55 fields, saw 65\\nSkipping line 790: expected 55 fields, saw 63\\nSkipping line 841: expected 55 fields, saw 63\\nSkipping line 931: expected 55 fields, saw 60\\nSkipping line 978: expected 55 fields, saw 69\\nSkipping line 1024: expected 55 fields, saw 60\\nSkipping line 1025: expected 55 fields, saw 63\\nSkipping line 1038: expected 55 fields, saw 60\\nSkipping line 1094: expected 55 fields, saw 58\\nSkipping line 1122: expected 55 fields, saw 60\\nSkipping line 1130: expected 55 fields, saw 56\\nSkipping line 1171: expected 55 fields, saw 58\\nSkipping line 1172: expected 55 fields, saw 60\\nSkipping line 1177: expected 55 fields, saw 56\\nSkipping line 1197: expected 55 fields, saw 60\\nSkipping line 1233: expected 55 fields, saw 60\\nSkipping line 1252: expected 55 fields, saw 60\\nSkipping line 1273: expected 55 fields, saw 60\\nSkipping line 1283: expected 55 fields, saw 60\\nSkipping line 1284: expected 55 fields, saw 60\\nSkipping line 1426: expected 55 fields, saw 60\\nSkipping line 1455: expected 55 fields, saw 56\\nSkipping line 1477: expected 55 fields, saw 60\\nSkipping line 1482: expected 55 fields, saw 60\\nSkipping line 1484: expected 55 fields, saw 61\\nSkipping line 1487: expected 55 fields, saw 60\\nSkipping line 1493: expected 55 fields, saw 57\\nSkipping line 1548: expected 55 fields, saw 58\\nSkipping line 1559: expected 55 fields, saw 60\\nSkipping line 1591: expected 55 fields, saw 60\\nSkipping line 1599: expected 55 fields, saw 91\\nSkipping line 1627: expected 55 fields, saw 60\\nSkipping line 1646: expected 55 fields, saw 60\\nSkipping line 1647: expected 55 fields, saw 60\\nSkipping line 1672: expected 55 fields, saw 60\\nSkipping line 1717: expected 55 fields, saw 69\\nSkipping line 1737: expected 55 fields, saw 78\\nSkipping line 1751: expected 55 fields, saw 60\\nSkipping line 1756: expected 55 fields, saw 60\\nSkipping line 1782: expected 55 fields, saw 60\\nSkipping line 1797: expected 55 fields, saw 60\\nSkipping line 1851: expected 55 fields, saw 60\\nSkipping line 1862: expected 55 fields, saw 60\\nSkipping line 1929: expected 55 fields, saw 60\\nSkipping line 1963: expected 55 fields, saw 60\\n'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the external file that stores the US states information\n",
        "state_df = pd.read_csv(\"/content/drive/MyDrive/us-state-list.csv\")\n",
        "state_abbr = state_df[\"Abbreviation\"].tolist()\n",
        "state_full = state_df[\"Full-Name\"].tolist()\n",
        "state_dict = dict(zip(state_abbr, state_full))"
      ],
      "metadata": {
        "id": "2asez2V0Prvu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the pure texts of the webpages\n",
        "with open(\"/content/drive/MyDrive/Cicero/cicero_officials_sample_2022-09-08_processed_raw_content.json\", \"r\") as f:\n",
        "      pure_text_dict = json.load(f)"
      ],
      "metadata": {
        "id": "36HQTy1gQx7w"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bio tagging"
      ],
      "metadata": {
        "id": "RTzRXLF2uIQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = Path(\"./mergeB/\")"
      ],
      "metadata": {
        "id": "ce_v76IXwQ1w"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# detect if the output directory exists\n",
        "if not os.path.exists(output):\n",
        "    os.makedirs(output)"
      ],
      "metadata": {
        "id": "aY-1HxPpxALe"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bio tagging\n",
        "# the output will be stored in the list first and then save as the spaCy Doc file\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "tagged_data = []\n",
        "\n",
        "for n in tqdm(range(len(cicero_df))):\n",
        "    information_unit = cicero_df.iloc[n].dropna()\n",
        "    politician_id = information_unit[\"id\"]\n",
        "\n",
        "    pure_text = pure_text_dict.get(str(politician_id), None)\n",
        "\n",
        "    # skip the data point that we cannot find the pure text due to some reasons\n",
        "    # for example, the url address is not valid\n",
        "    if pure_text is None:\n",
        "        continue\n",
        "\n",
        "    ruler = nlp.add_pipe(\"span_ruler\")\n",
        "    bio_tag_pattern_list = []\n",
        "    \n",
        "    # reference: https://spacy.io/usage/rule-based-matching\n",
        "    if \"salutation\" in information_unit.keys():\n",
        "        salutation = information_unit[\"salutation\"]\n",
        "        bio_tag_pattern_list.append({\"label\": \"SALUTATION\", \"pattern\": salutation})\n",
        "    \n",
        "    if \"party\" in information_unit.keys():\n",
        "        party = information_unit[\"party\"]\n",
        "        bio_tag_pattern_list.append({\"label\": \"PARTY\", \"pattern\": party})\n",
        "\n",
        "    if \"primary_state\" in information_unit.keys():\n",
        "        state = information_unit[\"primary_state\"]\n",
        "        bio_tag_pattern_list.append({\"label\": \"STATE\", \"pattern\": state})\n",
        "        # get the full name of the state\n",
        "        if state in state_abbr:\n",
        "            state_full_name = state_dict[state]\n",
        "            bio_tag_pattern_list.append({\"label\":'STATE_F', 'pattern':[{'LOWER':state_full_name.lower()}]})\n",
        "\n",
        "    if \"secondary_state\" in information_unit.keys():\n",
        "        state = information_unit[\"secondary_state\"]\n",
        "        bio_tag_pattern_list.append({\"label\": \"STATE\", \"pattern\": state})\n",
        "        # get the full name of the state\n",
        "        if state in state_abbr:\n",
        "            state_full_name = state_dict[state]\n",
        "            bio_tag_pattern_list.append({\"label\":'STATE_F', 'pattern':[{'LOWER':state_full_name.lower()}]})\n",
        "\n",
        "    if \"primary_county\" in information_unit.keys():\n",
        "        county = information_unit[\"primary_county\"]\n",
        "        bio_tag_pattern_list.append({\"label\": \"COUNTY\", \"pattern\": county})\n",
        "    \n",
        "    if \"secondary_county\" in information_unit.keys():\n",
        "        county = information_unit[\"secondary_county\"]\n",
        "        bio_tag_pattern_list.append({\"label\": \"COUNTY\", \"pattern\": county})\n",
        "\n",
        "    if \"primary_city\" in information_unit.keys():\n",
        "        city = information_unit[\"primary_city\"]\n",
        "        bio_tag_pattern_list.append({\"label\": \"CITY\", \"pattern\": city})\n",
        "\n",
        "    if \"secondary_city\" in information_unit.keys():\n",
        "        city = information_unit[\"secondary_city\"]\n",
        "        bio_tag_pattern_list.append({\"label\": \"CITY\", \"pattern\": city})\n",
        "    \n",
        "    ruler.add_patterns(bio_tag_pattern_list)\n",
        "    doc = nlp(pure_text)\n",
        "    length = len(doc)//100\n",
        "    for n in range(length+1):\n",
        "        sub_doc = nlp(str(doc[n*100:(n+1)*100]))\n",
        "        # the filter_spans function is used to remove the overlapping entities\n",
        "        sub_doc.ents = filter_spans(sub_doc.spans[\"ruler\"])\n",
        "        tagged_data.append(sub_doc)\n",
        "    \n",
        "    # remove the ruler and initialize a new one for each politician/data point\n",
        "    nlp.remove_pipe(\"span_ruler\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BUWo_NHuFYb",
        "outputId": "b18e8fad-0e42-4775-f4fc-f49ed68f42be"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1919/1919 [15:24<00:00,  2.08it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove the empty data point\n",
        "tagged_data = [doc for doc in tagged_data if len(doc) > 0]"
      ],
      "metadata": {
        "id": "OWGcPmgoUw06"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the tagged data into training set and test set\n",
        "train, dev = train_test_split(tagged_data, test_size = 0.1, random_state=42)"
      ],
      "metadata": {
        "id": "qmVn5k_6VRHQ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_db = DocBin()\n",
        "for n in train:\n",
        "    train_db.add(n)\n",
        "train_db.to_disk(output / \"train.spacy\")"
      ],
      "metadata": {
        "id": "yKF0LEjhVXAN"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_db = DocBin()\n",
        "for n in dev:\n",
        "    dev_db.add(n)\n",
        "dev_db.to_disk(output / \"dev.spacy\")"
      ],
      "metadata": {
        "id": "fqXSg2yjVgsK"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}