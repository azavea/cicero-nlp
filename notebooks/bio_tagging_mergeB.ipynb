{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbDlCCmnt8Dc"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqBAneo7xWr9"
      },
      "source": [
        "This code is the runnable python notebook version of `bio_tagging_mergeB.py`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu9fWXcWxUCX"
      },
      "source": [
        "This file is used to BIO tagging on the pure texts for the modelB, which covers information including salutation,\n",
        "party, state, county, and city."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtOJoGIlyjcn"
      },
      "source": [
        "The output file will be in the format of spaCy Doc file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg8WUFxzxmnH"
      },
      "source": [
        "special tricks applied: \n",
        "1. use the external file that stores the US states information to tag the abbreviation and full name of the state information.\n",
        "    the abbreviation will be tagged as \"STATE\", and the full name will be tagged as \"STATE_F\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZXzPph_MM5M"
      },
      "source": [
        "special dependencies:\n",
        "1. the external file that stores the US states information `state.csv`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhdO0xCLt88N"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhrcO56Ot4a6"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from spacy.tokens import Doc, DocBin\n",
        "from spacy.util import filter_spans\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR_HgmGvuF17"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your own path to cicero data\n",
        "CICERO_DATA_PATH = ''\n",
        "\n",
        "# your own path to the state file\n",
        "STATE_FILE_PATH = ''\n",
        "\n",
        "# your own path to the pure text file\n",
        "PURE_TEXT_PATH = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M569HBvHuUye"
      },
      "outputs": [],
      "source": [
        "# load the Cicero data\n",
        "cicero_df = pd.read_csv('CICERO_DATA_PATH',\\\n",
        "                  error_bad_lines=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2asez2V0Prvu"
      },
      "outputs": [],
      "source": [
        "# load the external file that stores the US states information\n",
        "state_df = pd.read_csv(\"STATE_FILE_PATH\")\n",
        "state_abbr = state_df[\"Abbreviation\"].tolist()\n",
        "state_full = state_df[\"Full-Name\"].tolist()\n",
        "state_dict = dict(zip(state_abbr, state_full))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36HQTy1gQx7w"
      },
      "outputs": [],
      "source": [
        "# load the pure texts of the webpages\n",
        "with open(\"PURE_TEXT_PATH \", \"r\") as f:\n",
        "      pure_text_dict = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTzRXLF2uIQl"
      },
      "source": [
        "## Bio Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce_v76IXwQ1w"
      },
      "outputs": [],
      "source": [
        "output = Path(\"./mergeB/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aY-1HxPpxALe"
      },
      "outputs": [],
      "source": [
        "# detect if the output directory exists\n",
        "if not os.path.exists(output):\n",
        "    os.makedirs(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BUWo_NHuFYb",
        "outputId": "1dd84f82-26e8-4727-cce0-0623b38d7a48"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1919/1919 [14:32<00:00,  2.20it/s]\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "# bio tagging\n",
        "# the output will be stored in the list first and then save as the spaCy Doc file\n",
        "tagged_data = []\n",
        "\n",
        "for n in tqdm(range(len(cicero_df))):\n",
        "    information_unit = cicero_df.iloc[n].dropna()\n",
        "    politician_id = information_unit[\"id\"]\n",
        "\n",
        "    pure_text = pure_text_dict.get(str(politician_id), None)\n",
        "\n",
        "    # skip the data point that we cannot find the pure text due to some reasons\n",
        "    # for example, the url address is not valid\n",
        "    if pure_text is None:\n",
        "        continue\n",
        "\n",
        "    ruler = nlp.add_pipe(\"span_ruler\")\n",
        "    bio_tag_pattern_list = []\n",
        "\n",
        "    # reference: https://spacy.io/usage/rule-based-matching\n",
        "    if \"salutation\" in information_unit.keys():\n",
        "        salutation = information_unit[\"salutation\"]\n",
        "        bio_tag_pattern_list.append({\n",
        "            \"label\": \"SALUTATION\",\n",
        "            \"pattern\": salutation\n",
        "        })\n",
        "\n",
        "    if \"party\" in information_unit.keys():\n",
        "        party = information_unit[\"party\"]\n",
        "        bio_tag_pattern_list.append({\"label\": \"PARTY\", \"pattern\": party})\n",
        "\n",
        "    if \"primary_state\" in information_unit.keys():\n",
        "        state = information_unit[\"primary_state\"]\n",
        "        bio_tag_pattern_list.append({\"label\": \"STATE\", \"pattern\": state})\n",
        "        # get the full name of the state\n",
        "        if state in state_abbr:\n",
        "            state_full_name = state_dict[state]\n",
        "            bio_tag_pattern_list.append({\n",
        "                \"label\": 'STATE_F',\n",
        "                'pattern': [{\n",
        "                    'LOWER': state_full_name.lower()\n",
        "                }]\n",
        "            })\n",
        "\n",
        "    if \"secondary_state\" in information_unit.keys():\n",
        "        state = information_unit[\"secondary_state\"]\n",
        "        bio_tag_pattern_list.append({\"label\": \"STATE\", \"pattern\": state})\n",
        "        # get the full name of the state\n",
        "        if state in state_abbr:\n",
        "            state_full_name = state_dict[state]\n",
        "            bio_tag_pattern_list.append({\n",
        "                \"label\": 'STATE_F',\n",
        "                'pattern': [{\n",
        "                    'LOWER': state_full_name.lower()\n",
        "                }]\n",
        "            })\n",
        "\n",
        "    if \"primary_county\" in information_unit.keys():\n",
        "        county = information_unit[\"primary_county\"]\n",
        "        bio_tag_pattern_list.append({\"label\": \"COUNTY\", \"pattern\": county})\n",
        "\n",
        "    if \"secondary_county\" in information_unit.keys():\n",
        "        county = information_unit[\"secondary_county\"]\n",
        "        bio_tag_pattern_list.append({\"label\": \"COUNTY\", \"pattern\": county})\n",
        "\n",
        "    if \"primary_city\" in information_unit.keys():\n",
        "        city = information_unit[\"primary_city\"]\n",
        "        bio_tag_pattern_list.append({\"label\": \"CITY\", \"pattern\": city})\n",
        "\n",
        "    if \"secondary_city\" in information_unit.keys():\n",
        "        city = information_unit[\"secondary_city\"]\n",
        "        bio_tag_pattern_list.append({\"label\": \"CITY\", \"pattern\": city})\n",
        "\n",
        "    ruler.add_patterns(bio_tag_pattern_list)\n",
        "    doc = nlp(pure_text)\n",
        "    # split the doc into multiple chunks since the input of the model should be\n",
        "    # less than 512 tokens\n",
        "    length = len(doc) // 100\n",
        "    for n in range(length + 1):\n",
        "        sub_doc = nlp(str(doc[n * 100:(n + 1) * 100]))\n",
        "        # the filter_spans function is used to remove the overlapping entities\n",
        "        sub_doc.ents = filter_spans(sub_doc.spans[\"ruler\"])\n",
        "        tagged_data.append(sub_doc)\n",
        "\n",
        "    # remove the ruler and initialize a new one for each politician/data point\n",
        "    nlp.remove_pipe(\"span_ruler\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWGcPmgoUw06"
      },
      "outputs": [],
      "source": [
        "# remove the empty data point\n",
        "tagged_data = [doc for doc in tagged_data if len(doc) > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmVn5k_6VRHQ"
      },
      "outputs": [],
      "source": [
        "# split the tagged data into training set and test set\n",
        "train, dev = train_test_split(tagged_data, test_size = 0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKF0LEjhVXAN"
      },
      "outputs": [],
      "source": [
        "train_db = DocBin()\n",
        "for n in train:\n",
        "    train_db.add(n)\n",
        "train_db.to_disk(output / \"train.spacy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqXSg2yjVgsK"
      },
      "outputs": [],
      "source": [
        "dev_db = DocBin()\n",
        "for n in dev:\n",
        "    dev_db.add(n)\n",
        "dev_db.to_disk(output / \"dev.spacy\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
