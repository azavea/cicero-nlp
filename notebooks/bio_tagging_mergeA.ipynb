{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Summary"
      ],
      "metadata": {
        "id": "UbDlCCmnt8Dc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is the runnable python notebook version of `bio_tagging_mergeA.py`"
      ],
      "metadata": {
        "id": "hqBAneo7xWr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This file is used to BIO tagging on the pure texts for the mergeModelA, which convers information including names,\n",
        "phone numbers and fax numbers"
      ],
      "metadata": {
        "id": "fu9fWXcWxUCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output file will be in the format of spaCy Doc file."
      ],
      "metadata": {
        "id": "EVgCS-fYymV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "special tricks applied: \n",
        "1. use the regex to tag the phone numbers and fax numbers with mutiple formats\n",
        "   the spacy ruler support tag with regex but the behavior is not exactly same as the vanila regex.\n",
        "   the vanila regex is used to solve the problem.\n"
      ],
      "metadata": {
        "id": "Zg8WUFxzxmnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "special dependencies:<br>\n",
        "NONE"
      ],
      "metadata": {
        "id": "uZXzPph_MM5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "DhdO0xCLt88N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BhrcO56Ot4a6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import spacy\n",
        "from spacy.util import filter_spans\n",
        "from spacy.tokens import Doc, DocBin\n",
        "import collections\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "yR_HgmGvuF17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the Cicero data\n",
        "cicero_df = pd.read_csv('/content/drive/MyDrive/Cicero/cicero_officials_sample_2022-09-08.csv',\\\n",
        "                  error_bad_lines=False)"
      ],
      "metadata": {
        "id": "M569HBvHuUye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the pure texts of the webpages\n",
        "with open(\"/content/drive/MyDrive/Cicero/cicero_officials_sample_2022-09-08_processed_raw_content.json\", \"r\") as f:\n",
        "      pure_text_dict = json.load(f)"
      ],
      "metadata": {
        "id": "36HQTy1gQx7w"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bio tagging"
      ],
      "metadata": {
        "id": "RTzRXLF2uIQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = Path(\"./mergeA/\")"
      ],
      "metadata": {
        "id": "ce_v76IXwQ1w"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# detect if the output directory exists\n",
        "if not os.path.exists(output):\n",
        "    os.makedirs(output)"
      ],
      "metadata": {
        "id": "aY-1HxPpxALe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function\n",
        "def regex_search(web_content, unit, search_attribute, label, pattern_list, error_log):\n",
        "  '''\n",
        "  use regex to search the information in the web content and add the search result\n",
        "  to the rule_pattern for bio tagging.\n",
        "\n",
        "  Args:\n",
        "    web_content: string. the content where we want to search information.\n",
        "    unit: dictionary. the dictionary contains all the attribute.\n",
        "    search_attribute: string. the attribute's name. \n",
        "    label: string. the name of the label for the attribute.\n",
        "    pattern_list: list. the pattern list which will be added to ruler.\n",
        "    error_log: defaultdict(dictionary). the dictionary to store the error instance.\n",
        "  \n",
        "  Returns:\n",
        "    None\n",
        "  '''\n",
        "  pattern_list.append({\"label\":label, 'pattern':unit[search_attribute]})\n",
        "  part_list = re.split('[^\\w]',unit[search_attribute])\n",
        "  try:\n",
        "    regex_pattern = r\"((\\(({})\\)(\\s){{0,1}}?)|(({})(-|.)))?({})(-|.)({})\".format(part_list[1],\\\n",
        "                                                                      part_list[1],\\\n",
        "                                                                      part_list[3],\\\n",
        "                                                                      part_list[4])\n",
        "    regex_pattern = r\"((\\(({})\\)(\\s){{0,1}}?)|(({})(-|.)))?({})(-|.)({})\".format(part_list[1],\\\n",
        "                                                                      part_list[1],\\\n",
        "                                                                      part_list[3],\\\n",
        "                                                                      part_list[4])\n",
        "    search_result = re.search(regex_pattern,web_content)\n",
        "    if search_result:\n",
        "      pattern_list.append({\"label\":'PHONE', 'pattern':search_result.group()})\n",
        "  except Exception as e:\n",
        "\n",
        "    politician_id = unit['id']\n",
        "    error_log[politician_id]['url'] = unit['url_1']\n",
        "    error_log[politician_id]['search_attribute'] = search_attribute\n",
        "    error_log[politician_id]['error'] = str(e)"
      ],
      "metadata": {
        "id": "_v9bpgP9drlx"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bio tagging\n",
        "# the output file will be in the list first and then save as the spaCy Doc file\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "tagged_data = []\n",
        "\n",
        "# create the error log\n",
        "error_log = collections.defaultdict(dict)\n",
        "\n",
        "for n in tqdm(range(len(cicero_df))):\n",
        "    information_unit = cicero_df.iloc[n].dropna()\n",
        "    politician_id = information_unit[\"id\"]\n",
        "\n",
        "    pure_text = pure_text_dict.get(str(politician_id), None)\n",
        "\n",
        "    # skip the data point that we cannot find the pure text due to some reasons\n",
        "    # for example, the url address is not valid\n",
        "    if pure_text is None:\n",
        "        continue\n",
        "\n",
        "    ruler = nlp.add_pipe(\"span_ruler\")\n",
        "    bio_tag_pattern_list = []\n",
        "\n",
        "    # tag the name\n",
        "    # tag the full name, case insensitive by using the `LOWER` attribute\n",
        "    bio_tag_pattern_list.append({\"label\":'NAME', 'pattern':[{'LOWER':information_unit['first_name'].lower()},\\\n",
        "                                                            {'LOWER':information_unit['last_name'].lower()}]})\n",
        "    # tag the first name, case insensitive by using the `LOWER` attribute\n",
        "    bio_tag_pattern_list.append({\"label\":'NAME', 'pattern':[{'LOWER':information_unit['first_name'].lower()}]})\n",
        "    # tag the last name, case insensitive by using the `LOWER` attribute\n",
        "    bio_tag_pattern_list.append({\"label\":'NAME', 'pattern':[{'LOWER':information_unit['last_name'].lower()}]})\n",
        "\n",
        "    # tag the phone number\n",
        "    if 'primary_phone_1' in information_unit.keys():\n",
        "        regex_search(pure_text, information_unit, 'primary_phone_1', 'PHONE', bio_tag_pattern_list, error_log)\n",
        "    \n",
        "    if 'primary_phone_2' in information_unit.keys():\n",
        "        regex_search(pure_text, information_unit, 'primary_phone_2', 'PHONE', bio_tag_pattern_list, error_log)\n",
        "\n",
        "    if 'secondary_phone_1' in information_unit.keys():\n",
        "        regex_search(pure_text, information_unit, 'secondary_phone_1', 'PHONE', bio_tag_pattern_list, error_log)\n",
        "    \n",
        "    if 'secondary_phone_2' in information_unit.keys():\n",
        "        regex_search(pure_text, information_unit, 'secondary_phone_2', 'PHONE', bio_tag_pattern_list, error_log)\n",
        "    \n",
        "    # tag the fax number\n",
        "    if 'primary_fax_1' in information_unit.keys():\n",
        "        regex_search(pure_text, information_unit, 'primary_fax_1', 'FAX', bio_tag_pattern_list, error_log)\n",
        "    \n",
        "    if 'primary_fax_2' in information_unit.keys():\n",
        "        regex_search(pure_text, information_unit, 'primary_fax_2', 'FAX', bio_tag_pattern_list, error_log)\n",
        "    \n",
        "    if 'secondary_fax_1' in information_unit.keys():\n",
        "        regex_search(pure_text, information_unit, 'secondary_fax_1', 'FAX', bio_tag_pattern_list, error_log)\n",
        "    \n",
        "    if 'secondary_fax_2' in information_unit.keys():\n",
        "        regex_search(pure_text, information_unit, 'secondary_fax_2', 'FAX', bio_tag_pattern_list, error_log)\n",
        "\n",
        "    ruler.add_patterns(bio_tag_pattern_list)\n",
        "    doc = nlp(pure_text)\n",
        "    length = len(doc)//100\n",
        "    for n in range(length+1):\n",
        "        sub_doc = nlp(str(doc[n*100:(n+1)*100]))\n",
        "        sub_doc.ents = filter_spans(sub_doc.spans[\"ruler\"])\n",
        "        tagged_data.append(sub_doc)\n",
        "    \n",
        "    # remove the ruler and initialize a new one for each politician/data point\n",
        "    nlp.remove_pipe('span_ruler')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BUWo_NHuFYb",
        "outputId": "1c15f65e-5b43-44ac-bac2-39623de6d0e5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1919/1919 [16:19<00:00,  1.96it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.remove_pipe('span_ruler')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ogmfpr-zeheh",
        "outputId": "440be140-a8b0-46d1-8db5-87551f7652b0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('span_ruler', <spacy.pipeline.span_ruler.SpanRuler at 0x7f9339899980>)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove the empty data point\n",
        "tagged_data = [doc for doc in tagged_data if len(doc) > 0]"
      ],
      "metadata": {
        "id": "OWGcPmgoUw06"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the tagged data into training set and test set\n",
        "train, dev = train_test_split(tagged_data, test_size = 0.1, random_state=42)"
      ],
      "metadata": {
        "id": "qmVn5k_6VRHQ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_db = DocBin()\n",
        "for n in train:\n",
        "    train_db.add(n)\n",
        "train_db.to_disk(output / \"train.spacy\")"
      ],
      "metadata": {
        "id": "yKF0LEjhVXAN"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_db = DocBin()\n",
        "for n in dev:\n",
        "    dev_db.add(n)\n",
        "dev_db.to_disk(output / \"dev.spacy\")"
      ],
      "metadata": {
        "id": "fqXSg2yjVgsK"
      },
      "execution_count": 37,
      "outputs": []
    }
  ]
}