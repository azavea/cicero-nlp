{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbDlCCmnt8Dc"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqBAneo7xWr9"
      },
      "source": [
        "This code is the runnable python notebook version of `bio_tagging_mergeA.py`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu9fWXcWxUCX"
      },
      "source": [
        "This file is used to BIO tagging on the pure texts for the mergeModelA, which covers information including names,\n",
        "phone numbers and fax numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVgCS-fYymV9"
      },
      "source": [
        "The output file will be in the format of spaCy Doc file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg8WUFxzxmnH"
      },
      "source": [
        "special tricks applied: \n",
        "1. use the regex to tag the phone numbers and fax numbers with mutiple formats\n",
        "   the spacy ruler support tag with regex but the behavior is not exactly same as the vanila regex.<br>\n",
        "   the vanila regex is used to solve the problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZXzPph_MM5M"
      },
      "source": [
        "special dependencies:<br>\n",
        "NONE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhdO0xCLt88N"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhrcO56Ot4a6"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from spacy.tokens import Doc, DocBin\n",
        "from spacy.util import filter_spans\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR_HgmGvuF17"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your own path to cicero data\n",
        "CICERO_DATA_PATH = ''\n",
        "\n",
        "# your own path to the pure text file\n",
        "PURE_TEXT_PATH = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M569HBvHuUye"
      },
      "outputs": [],
      "source": [
        "# load the Cicero data\n",
        "cicero_df = pd.read_csv('CICERO_DATA_PATH',\\\n",
        "                  error_bad_lines=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36HQTy1gQx7w"
      },
      "outputs": [],
      "source": [
        "# load the pure texts of the webpages\n",
        "with open(\"PURE_TEXT_PATH\", \"r\") as f:\n",
        "      pure_text_dict = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTzRXLF2uIQl"
      },
      "source": [
        "## Bio tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce_v76IXwQ1w"
      },
      "outputs": [],
      "source": [
        "output = Path(\"./mergeA/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aY-1HxPpxALe"
      },
      "outputs": [],
      "source": [
        "# detect if the output directory exists\n",
        "if not os.path.exists(output):\n",
        "    os.makedirs(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_v9bpgP9drlx"
      },
      "outputs": [],
      "source": [
        "# helper function\n",
        "def regex_search(web_content, unit, search_attribute, label, pattern_list,\n",
        "                 error_log):\n",
        "    \"\"\"\n",
        "    use regex to search the information in the web content and add the search result\n",
        "    to the rule_pattern for bio tagging.\n",
        "\n",
        "    Args:\n",
        "        web_content: the content where we want to search information.\n",
        "        unit: the dictionary contains all the attribute.\n",
        "        search_attribute: the attribute's name. \n",
        "        label: the name of the label for the attribute.\n",
        "        pattern_list: the pattern list which will be added to ruler.\n",
        "        error_log:the dictionary to store the error instance.\n",
        "    \n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    pattern_list.append({\"label\": label, 'pattern': unit[search_attribute]})\n",
        "    part_list = re.split('[^\\w]', unit[search_attribute])\n",
        "    try:\n",
        "        regex_pattern = r\"((\\(({})\\)(\\s){{0,1}}?)|(({})(-|.)))?({})(-|.)({})\".format(part_list[1],\\\n",
        "                                                                          part_list[1],\\\n",
        "                                                                          part_list[3],\\\n",
        "                                                                          part_list[4])\n",
        "\n",
        "        search_result = re.search(regex_pattern, web_content)\n",
        "        if search_result:\n",
        "            pattern_list.append({\n",
        "                \"label\": 'PHONE',\n",
        "                'pattern': search_result.group()\n",
        "            })\n",
        "    except Exception as e:\n",
        "\n",
        "        politician_id = unit['id']\n",
        "\n",
        "        error_log[politician_id]['url'] = unit['url_1']\n",
        "        error_log[politician_id]['search_attribute'] = search_attribute\n",
        "        error_log[politician_id]['error'] = str(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BUWo_NHuFYb",
        "outputId": "156a8a11-3506-4af0-bfa2-e0e7f7f1548e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1919/1919 [15:47<00:00,  2.03it/s]\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "# bio tagging\n",
        "# the output file will be in the list first and then save as the spaCy Doc file\n",
        "tagged_data = []\n",
        "\n",
        "# create the error log\n",
        "error_log = collections.defaultdict(dict)\n",
        "\n",
        "for n in tqdm(range(len(cicero_df))):\n",
        "    information_unit = cicero_df.iloc[n].dropna()\n",
        "    politician_id = information_unit[\"id\"]\n",
        "\n",
        "    pure_text = pure_text_dict.get(str(politician_id), None)\n",
        "\n",
        "    # skip the data point that we cannot find the pure text due to some reasons\n",
        "    # for example, the url address is not valid\n",
        "    if pure_text is None:\n",
        "        continue\n",
        "\n",
        "    ruler = nlp.add_pipe(\"span_ruler\")\n",
        "    bio_tag_pattern_list = []\n",
        "\n",
        "    # reference: https://spacy.io/usage/rule-based-matching\n",
        "    # tag the name\n",
        "    # tag the full name, case insensitive by using the `LOWER` attribute\n",
        "    bio_tag_pattern_list.append({\n",
        "        \"label\":\n",
        "            'NAME',\n",
        "        'pattern': [{\n",
        "            'LOWER': information_unit['first_name'].lower()\n",
        "        }, {\n",
        "            'LOWER': information_unit['last_name'].lower()\n",
        "        }]\n",
        "    })\n",
        "    # tag the first name, case insensitive by using the `LOWER` attribute\n",
        "    bio_tag_pattern_list.append({\n",
        "        \"label\": 'NAME',\n",
        "        'pattern': [{\n",
        "            'LOWER': information_unit['first_name'].lower()\n",
        "        }]\n",
        "    })\n",
        "    # tag the last name, case insensitive by using the `LOWER` attribute\n",
        "    bio_tag_pattern_list.append({\n",
        "        \"label\": 'NAME',\n",
        "        'pattern': [{\n",
        "            'LOWER': information_unit['last_name'].lower()\n",
        "        }]\n",
        "    })\n",
        "\n",
        "    # tag the phone number\n",
        "    if 'primary_phone_1' in information_unit.keys():\n",
        "        regex_search(pure_text, information_unit, 'primary_phone_1',\n",
        "                      'PHONE', bio_tag_pattern_list, error_log)\n",
        "\n",
        "    if 'primary_phone_2' in information_unit.keys():\n",
        "        regex_search(pure_text, information_unit, 'primary_phone_2',\n",
        "                      'PHONE', bio_tag_pattern_list, error_log)\n",
        "\n",
        "    if 'secondary_phone_1' in information_unit.keys():\n",
        "        regex_search(pure_text, information_unit, 'secondary_phone_1',\n",
        "                      'PHONE', bio_tag_pattern_list, error_log)\n",
        "\n",
        "    if 'secondary_phone_2' in information_unit.keys():\n",
        "        regex_search(pure_text, information_unit, 'secondary_phone_2',\n",
        "                      'PHONE', bio_tag_pattern_list, error_log)\n",
        "\n",
        "    # tag the fax number\n",
        "    if 'primary_fax_1' in information_unit.keys():\n",
        "        regex_search(pure_text, information_unit, 'primary_fax_1', 'FAX',\n",
        "                      bio_tag_pattern_list, error_log)\n",
        "\n",
        "    if 'primary_fax_2' in information_unit.keys():\n",
        "        regex_search(pure_text, information_unit, 'primary_fax_2', 'FAX',\n",
        "                      bio_tag_pattern_list, error_log)\n",
        "\n",
        "    if 'secondary_fax_1' in information_unit.keys():\n",
        "        regex_search(pure_text, information_unit, 'secondary_fax_1', 'FAX',\n",
        "                      bio_tag_pattern_list, error_log)\n",
        "\n",
        "    if 'secondary_fax_2' in information_unit.keys():\n",
        "        regex_search(pure_text, information_unit, 'secondary_fax_2', 'FAX',\n",
        "                      bio_tag_pattern_list, error_log)\n",
        "\n",
        "    ruler.add_patterns(bio_tag_pattern_list)\n",
        "    doc = nlp(pure_text)\n",
        "    # split the doc into multiple chunks since the input of the model should be\n",
        "    # less than 512 tokens\n",
        "    length = len(doc) // 100\n",
        "    for n in range(length + 1):\n",
        "        sub_doc = nlp(str(doc[n * 100:(n + 1) * 100]))\n",
        "        sub_doc.ents = filter_spans(sub_doc.spans[\"ruler\"])\n",
        "        tagged_data.append(sub_doc)\n",
        "\n",
        "    # remove the ruler and initialize a new one for each politician/data point\n",
        "    nlp.remove_pipe('span_ruler')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWGcPmgoUw06"
      },
      "outputs": [],
      "source": [
        "# remove the empty data point\n",
        "tagged_data = [doc for doc in tagged_data if len(doc) > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmVn5k_6VRHQ"
      },
      "outputs": [],
      "source": [
        "# split the tagged data into training set and test set\n",
        "train, dev = train_test_split(tagged_data, test_size = 0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKF0LEjhVXAN"
      },
      "outputs": [],
      "source": [
        "train_db = DocBin()\n",
        "for n in train:\n",
        "    train_db.add(n)\n",
        "train_db.to_disk(output / \"train.spacy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqXSg2yjVgsK"
      },
      "outputs": [],
      "source": [
        "dev_db = DocBin()\n",
        "for n in dev:\n",
        "    dev_db.add(n)\n",
        "dev_db.to_disk(output / \"dev.spacy\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
